{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89dc1c2-cff5-460f-ba69-f47708b5ff93",
   "metadata": {},
   "source": [
    "# Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df71d54-08ce-43d4-8364-531696a8feff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/user/miniconda/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /home/user/miniconda/lib/python3.9/site-packages (4.41.0)\n",
      "Requirement already satisfied: torch in /home/user/miniconda/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/user/miniconda/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/user/miniconda/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/miniconda/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/user/miniconda/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32eb3eb-59af-43aa-92de-6b0870b4bc37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b26426a1-9f61-4140-ad01-eb6f7622e59d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 04:29:44,245 INFO -- datasets: PyTorch version 2.2.2 available.\n",
      "/home/user/miniconda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset, Audio\n",
    "from transformers import AutoProcessor, SeamlessM4TModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac555aba-188a-495d-922e-4d25262106ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and Prepare Data\n",
    "We will load the CSV files into pandas dataframes, add full paths to the filenames, and convert the dataframes to Hugging Face datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237663d9-bbb9-45a8-aecb-6acbd678607b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescue Team DataFrame Sample:                                        filename    translation_arabic  \\\n",
      "0  dataset_amazigh/wav/rescue_wav/S1_resc_1.wav       هل الجميع بخير؟   \n",
      "1  dataset_amazigh/wav/rescue_wav/S1_resc_2.wav  هل تحتاج إلى مساعدة؟   \n",
      "2  dataset_amazigh/wav/rescue_wav/S1_resc_3.wav           أين إصابتك؟   \n",
      "3  dataset_amazigh/wav/rescue_wav/S1_resc_4.wav     نحن هنا للمساعدة.   \n",
      "4  dataset_amazigh/wav/rescue_wav/S1_resc_7.wav       نحتاج إلى حملك.   \n",
      "\n",
      "      translation_english  \n",
      "0       Is everyone okay?  \n",
      "1       Do you need help?  \n",
      "2  Where are you injured?  \n",
      "3    We are here to help.  \n",
      "4   We need to carry you.  \n",
      "Small Talk DataFrame Sample:                                      filename        translation_arabic  \\\n",
      "0  dataset_amazigh/wav/conv_wav/S1_conv_1.wav         مرحبًا! كيف حالك؟   \n",
      "1  dataset_amazigh/wav/conv_wav/S1_conv_2.wav    أنا بخير، شكرًا. وأنت؟   \n",
      "2  dataset_amazigh/wav/conv_wav/S1_conv_3.wav                  ما اسمك؟   \n",
      "3  dataset_amazigh/wav/conv_wav/S1_conv_4.wav  اسمي [اسم]. سعيد بلقائك.   \n",
      "4  dataset_amazigh/wav/conv_wav/S1_conv_5.wav               من أين أنت؟   \n",
      "\n",
      "                    translation_english  \n",
      "0                   Hello! How are you?  \n",
      "1         I'm fine, thank you. And you?  \n",
      "2                     What's your name?  \n",
      "3  My name is [Name]. Nice to meet you.  \n",
      "4                   Where are you from?  \n"
     ]
    }
   ],
   "source": [
    "rescue_team_df = pd.read_csv('dataset_amazigh/annotations/rescue_team.csv')\n",
    "small_talk_df = pd.read_csv('dataset_amazigh/annotations/small_talk.csv')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Drop rows where 'filename' or 'translation_english' is NaN\n",
    "    df = df.dropna(subset=['filename', 'translation_english'])\n",
    "    return df\n",
    "\n",
    "    \n",
    "# Define the base directories for the audio files\n",
    "rescue_base_dir = 'dataset_amazigh/wav/rescue_wav/'\n",
    "small_talk_base_dir = 'dataset_amazigh/wav/conv_wav/'\n",
    "\n",
    "rescue_team_df['filename'] = rescue_base_dir + rescue_team_df['filename']\n",
    "small_talk_df['filename'] = small_talk_base_dir + small_talk_df['filename']\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "rescue_team_df = preprocess_data(rescue_team_df)\n",
    "small_talk_df = preprocess_data(small_talk_df)\n",
    "\n",
    "# Print first few rows to debug\n",
    "print(\"Rescue Team DataFrame Sample:\", rescue_team_df.head())\n",
    "print(\"Small Talk DataFrame Sample:\", small_talk_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19943ba2-ac73-4f64-9c83-ee963102b6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Train Dataset size: 130\n",
      "Combined Test Dataset size: 130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to Hugging Face datasets\n",
    "def convert_to_dataset(df):\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "rescue_team_dataset = convert_to_dataset(rescue_team_df)\n",
    "small_talk_dataset = convert_to_dataset(small_talk_df)\n",
    "\n",
    "# Adding audio path to the datasets\n",
    "rescue_team_dataset = rescue_team_dataset.cast_column(\"filename\", Audio(sampling_rate=16000))\n",
    "small_talk_dataset = small_talk_dataset.cast_column(\"filename\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Combine datasets\n",
    "combined_train_dataset = concatenate_datasets([rescue_team_dataset, small_talk_dataset])\n",
    "combined_test_dataset = concatenate_datasets([rescue_team_dataset, small_talk_dataset])\n",
    "\n",
    "# Check combined dataset sizes\n",
    "print(f\"Combined Train Dataset size: {len(combined_train_dataset)}\")\n",
    "print(f\"Combined Test Dataset size: {len(combined_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d176201e-bd63-49e9-9f44-8c6b796f0843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'translation_arabic', 'translation_english'],\n",
      "        num_rows: 130\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'translation_arabic', 'translation_english'],\n",
      "        num_rows: 130\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DatasetDict\n",
    "combined_dataset = DatasetDict({\n",
    "    \"train\": combined_train_dataset,\n",
    "    \"test\": combined_test_dataset  \n",
    "})\n",
    "\n",
    "# Print dataset info to debug\n",
    "print(\"Combined Dataset:\", combined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c094aa-333f-4fa0-9a76-c26119ec267e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Tokenizer and Model\n",
    "We will define the tokenizer and model using the Seamless M4T model from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b13978d-68e9-42d8-b084-019436017ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Seamless M4T processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n",
    "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f798079-e687-434b-85e4-07c7713f5664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/user/miniconda/lib/python3.9/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in /home/user/miniconda/lib/python3.9/site-packages (0.12.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (0.59.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/user/miniconda/lib/python3.9/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/user/miniconda/lib/python3.9/site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /home/user/miniconda/lib/python3.9/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Requirement already satisfied: packaging in /home/user/miniconda/lib/python3.9/site-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/user/miniconda/lib/python3.9/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/user/miniconda/lib/python3.9/site-packages (from pooch>=1.1->librosa) (4.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/user/miniconda/lib/python3.9/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2021.5.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b4023d4-198c-4956-9b84-9c1566a2d559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/user/miniconda/lib/python3.9/site-packages (4.41.0)\n",
      "Requirement already satisfied: datasets in /home/user/miniconda/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: sentencepiece in /home/user/miniconda/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9dde2e8-f737-40f9-ad75-2a4d47683d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/user/miniconda/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /home/user/miniconda/lib/python3.9/site-packages (4.41.0)\n",
      "Requirement already satisfied: sentencepiece in /home/user/miniconda/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec7adf-f5d7-4bed-96cd-b4a697db9dc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocess Data\n",
    "We need to define a preprocessing function to tokenize the inputs and labels.\n",
    "before that we need to make the audios in the same length by adding padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f3d935-0aae-47e1-adde-6c9549a01650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3467404d2244cbbcd6ade028f1848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_amazigh/wav/rescue_wav/S1_resc_1.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_3.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_4.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_8.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_10.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_11.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_12.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_13.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_14.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_15.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_16.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_17.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_18.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_19.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_20.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_22.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_23.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_24.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_26.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_27.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_30.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_31.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_32.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_33.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_35.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_36.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_37.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_38.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_39.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_41.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_42.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_5.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_8.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_10.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_11.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_12.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_14.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_16.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_17.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_20.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_1.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_3.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_6.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_19.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_22.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_24.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_25.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_26.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_27.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_30.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_31.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_32.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_35.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_36.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_37.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_38.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_39.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_5.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_11.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_15.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_16.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_17.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_18.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_19.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_20.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_21.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_22.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_23.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_24.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_25.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_26.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_28.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_29.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_30.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_31.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_32.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_33.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_35.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_37.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_38.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_39.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_40.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_41.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_5.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_13.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_16.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_17.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_13.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_14.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_16.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6174b4dbf697423d9f5f4a558bd4fa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_amazigh/wav/rescue_wav/S1_resc_1.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_3.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_4.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_8.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_10.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_11.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_12.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_13.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_14.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_15.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_16.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_17.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_18.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_19.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_20.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_22.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_23.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_24.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_26.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_27.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_30.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_31.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_32.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_33.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_35.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_36.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_37.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_38.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_39.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_41.wav\n",
      "dataset_amazigh/wav/rescue_wav/S1_resc_42.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_5.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_8.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_10.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_11.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_12.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_14.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_16.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_17.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_20.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S2_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_1.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_3.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_6.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_7.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_9.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_19.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_21.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_22.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_24.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_25.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_26.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_27.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_28.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_30.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_31.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_32.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_35.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_36.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_37.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_38.wav\n",
      "dataset_amazigh/wav/rescue_wav/S3_resc_39.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_5.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_11.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_15.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_16.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_17.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_18.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_19.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_20.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_21.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_22.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_23.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_24.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_25.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_26.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_28.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_29.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_30.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_31.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_32.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_33.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_35.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_37.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_38.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_39.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_40.wav\n",
      "dataset_amazigh/wav/conv_wav/S1_conv_41.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_5.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_13.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_16.wav\n",
      "dataset_amazigh/wav/conv_wav/S2_conv_17.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_1.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_2.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_3.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_4.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_6.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_7.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_8.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_9.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_10.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_13.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_14.wav\n",
      "dataset_amazigh/wav/conv_wav/S3_conv_16.wav\n",
      "Min length: 18845\n",
      "Max length: 87576\n",
      "Mean length: 47471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets, Audio\n",
    "from transformers import AutoProcessor, Wav2Vec2ForCTC, TrainingArguments, Seq2SeqTrainer\n",
    "import librosa\n",
    "import numpy as np\n",
    "# Use a valid model identifier from the Hugging Face model hub\n",
    "model_name_or_path = 'facebook/wav2vec2-base-960h'\n",
    "\n",
    "# Initialize the processor\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Initialize the model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Calculate lengths of all audio files\n",
    "lengths = []\n",
    "\n",
    "def calculate_length(examples):\n",
    "    print(examples[\"filename\"]['path'])\n",
    "    audio_path = examples[\"filename\"]['path']\n",
    "    audio_array, _ = librosa.load(audio_path, sr=16000)\n",
    "    lengths.append(len(audio_array))\n",
    "    return examples\n",
    "\n",
    "combined_dataset.map(calculate_length, batched=False)\n",
    "\n",
    "# Compute statistics\n",
    "min_length = min(lengths)\n",
    "max_length = max(lengths)\n",
    "mean_length = int(np.mean(lengths))\n",
    "\n",
    "print(f\"Min length: {min_length}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Mean length: {mean_length}\")\n",
    "\n",
    "# Define the chosen length for padding (for example, mean length)\n",
    "chosen_length = max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f41352a7-58b3-4454-acfa-679a50b83b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87576"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de91e797-6350-4cea-a90f-4559b0e0ec1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mapping preprocess_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing function with padding\n",
    "def preprocess_function(examples):\n",
    "    audio_paths = examples[\"filename\"]\n",
    "    inputs = {\"input_values\": [], \"labels\": []}\n",
    "    for audio_path, text in zip(audio_paths, examples[\"translation_english\"]):\n",
    "        # Load and process the audio file\n",
    "        \n",
    "        audio_array, sampling_rate = librosa.load(audio_path['path'], sr=16000)\n",
    "        \n",
    "        # Pad the audio array to the chosen length\n",
    "        if len(audio_array) > chosen_length:\n",
    "            audio_array = audio_array[:chosen_length]\n",
    "        else:\n",
    "            audio_array = np.pad(audio_array, (0, chosen_length - len(audio_array)), 'constant')\n",
    "        \n",
    "        input_values = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\",padding= True).input_values[0]\n",
    "        inputs[\"input_values\"].append(input_values.numpy())\n",
    "        \n",
    "        # Ensure the text input is in the correct format (str or List[str])\n",
    "        if isinstance(text, str):\n",
    "            text = [text]  # Convert single string to list\n",
    "        elif isinstance(text, list) and isinstance(text[0], list):\n",
    "            text = [' '.join(t) for t in text]  # Flatten nested lists\n",
    "        \n",
    "        # Process text input to labels\n",
    "        labels = processor(text=text, return_tensors=\"pt\", padding=True).input_ids[0]\n",
    "        inputs[\"labels\"].append(labels.numpy())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "print(\"Before mapping preprocess_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bef282a-7b8b-4b76-bc15-1de6e7a1aabc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473731e52e92476abe7b0e7ba075b12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset preprocessing successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c4b9a92f7d41859254ab32cf7eb5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset preprocessing successful\n",
      "After mapping preprocess_function\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenized_train_dataset = combined_train_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=combined_train_dataset.column_names\n",
    "    )\n",
    "    print(\"Training dataset preprocessing successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training dataset preprocessing: {e}\")\n",
    "    tokenized_train_dataset = None\n",
    "\n",
    "try:\n",
    "    tokenized_eval_dataset = combined_test_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=combined_test_dataset.column_names\n",
    "    )\n",
    "    print(\"Evaluation dataset preprocessing successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation dataset preprocessing: {e}\")\n",
    "    tokenized_eval_dataset = None\n",
    "\n",
    "print(\"After mapping preprocess_function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f9b72cf-e80c-495a-b59d-97492e76d322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'translation_arabic', 'translation_english'],\n",
       "    num_rows: 130\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83b5c2-7a36-4cd9-989d-50f479a3c37b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Training Arguments and Metrics\n",
    "We will set the training arguments for fine-tuning the model and define Word Error Rate (WER) as the evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c93551-7695-4983-b73d-107d78b455c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2ForCTC, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# Define the evaluation metric\n",
    "wer_metric = evaluate.load(\"wer\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions.argmax(-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_ids = pred.label_ids\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "608f2704-6614-4702-ac20-8345387e964c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import Wav2Vec2Processor\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5215f5-40c2-48e9-86cf-e04a4523fe92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune the Model\n",
    "We will start the fine-tuning process using the trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae458382-13ca-4bfb-921b-91fa3cba9146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "after\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3237.006348</td>\n",
       "      <td>1.007364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2874.892334</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2657.458740</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if both datasets are successfully processed\n",
    "if tokenized_train_dataset and tokenized_eval_dataset:\n",
    "    # Initialize training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    print(\"before\")\n",
    "    # Initialize the data collator\n",
    "    data_collator= DataCollatorCTCWithPadding(processor, padding= True)\n",
    "    # Initialize the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"after\")\n",
    "\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Word Error Rate: {results['eval_wer']:.2f}\")\n",
    "else:\n",
    "    print(\"Dataset not available for preprocessing and training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7528a8-2542-4e9b-9894-056ad30a0ea5",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "After fine-tuning, we will save the model and processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fdc1ab8-0ff5-49f4-acaf-02756e321b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-seamless-m4t\")\n",
    "processor.save_pretrained(\"./fine-tuned-seamless-m4t\")\n",
    "\n",
    "print(\"Model and processor saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af730700-09da-44ba-990d-7e6439dd7dde",
   "metadata": {},
   "source": [
    "# Testing with some wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27d0a16b-c7b2-48a1-8b32-38e740baaefd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation for dataset_amazigh/wav/conv_wav/S3_conv_1.wav: SEDEMONICL ANCONTUGITE\n",
      "Translation for dataset_amazigh/wav/rescue_wav/S1_resc_2.wav: IELESMAQITAN\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "\n",
    "# Load the fine-tuned model and processor\n",
    "model_path = \"./fine-tuned-seamless-m4t\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Function to translate audio files\n",
    "def translate_audio(audio_path):\n",
    "    # Load the audio file\n",
    "    audio_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Preprocess the audio file\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "    \n",
    "    # Decode the predictions\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    translation = processor.batch_decode(predicted_ids)\n",
    "    \n",
    "    return translation[0]\n",
    "# List of test audio files (paths from your dataset)\n",
    "test_audio_files = [\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\",\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\"\n",
    "]\n",
    "\n",
    "# Translate and print the results\n",
    "for audio_file in test_audio_files:\n",
    "    translation = translate_audio(audio_file)\n",
    "    print(f\"Translation for {audio_file}: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314384a-19a9-444c-a9be-edad246d18f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be9a06be-1019-4f02-973d-2cf45b079589",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fairseq2\n",
    "!pip install pydub sentencepiece\n",
    "!pip install git+https://github.com/facebookresearch/seamless_communication.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb2d278f-7707-4ca9-ae23-201e4b82a427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: dataset_amazigh/wav/conv_wav/S3_conv_1.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: SEDAMONICO AN CONTUGIT\n",
      "\n",
      "Audio file: dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: IEREMAQITON\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mmap\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Load the fine-tuned model and processor from the specified local path\n",
    "model_path = \"./fine-tuned-seamless-m4t\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "\n",
    "# List of test audio files (paths from your dataset)\n",
    "test_audio_files = [\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\",\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\"\n",
    "]\n",
    "\n",
    "# Function to resample audio to 16kHz if necessary\n",
    "def resample_audio(audio_path, target_sr=16000):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio)\n",
    "    return audio, target_sr\n",
    "\n",
    "# Function to perform direct Amazigh-to-Arabic translation using the loaded model and processor\n",
    "def translate_audio(audio_path):\n",
    "    # Load the audio file\n",
    "    audio_array, sampling_rate = torchaudio.load(audio_path, normalize=True)\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "        audio_array = resampler(audio_array)\n",
    "    \n",
    "    # Preprocess the audio file\n",
    "    inputs = processor(audio_array.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "    \n",
    "    # Decode the predictions\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    translation = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return translation[0]\n",
    "\n",
    "# Translate and display the results\n",
    "for audio_file in test_audio_files:\n",
    "    print(f\"Audio file: {audio_file}\")\n",
    "    display(Audio(audio_file, rate=16000, autoplay=False, normalize=True))\n",
    "\n",
    "    # Perform direct translation from Amazigh to Arabic\n",
    "    translation = translate_audio(audio_file)\n",
    "    \n",
    "    print(f\"Translated text: {translation}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d2c9c2b-35b0-4d4a-b921-ca1c6c2e70c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model_name_or_card: Union[str, fairseq2.assets.card.AssetCard], vocoder_name_or_card: Union[str, fairseq2.assets.card.AssetCard, NoneType], device: torch.device, text_tokenizer: Optional[fairseq2.data.text.text_tokenizer.TextTokenizer] = None, apply_mintox: bool = False, dtype: torch.dtype = torch.float16, input_modality: Optional[seamless_communication.inference.translator.Modality] = None, output_modality: Optional[seamless_communication.inference.translator.Modality] = None)\n"
     ]
    }
   ],
   "source": [
    "from seamless_communication.inference import Translator\n",
    "import inspect\n",
    "\n",
    "# Inspect the __init__ method of the Translator class to see its parameters\n",
    "print(inspect.signature(Translator.__init__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b0323-1075-41e3-912c-1a03ee24f52a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Test/Demo on Inference Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39873000-6975-479e-bfee-2e7c05a6b105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_v2. Set `force` to `True` to download again.\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: dataset_amazigh/wav/conv_wav/S1_conv_37.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text in arb: استثنيت من المليئة لأنني لم أجد ثنيان\n",
      "\n",
      "\n",
      "Audio file: dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text in arb: لست بحاجة لتعاونك.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mmap\n",
    "import numpy\n",
    "import soundfile\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from seamless_communication.inference import Translator\n",
    "from seamless_communication.streaming.dataloaders.s2tt import SileroVADSilenceRemover\n",
    "\n",
    "# Initialize a Translator object with the correct parameters\n",
    "model_name = \"seamlessM4T_v2_large\"\n",
    "vocoder_name = \"vocoder_v2\" if model_name == \"seamlessM4T_v2_large\" else \"vocoder_36langs\"\n",
    "\n",
    "translator = Translator(\n",
    "    model_name_or_card=model_name,\n",
    "    vocoder_name_or_card=vocoder_name,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Define target languages\n",
    "tgt_langs = [\"arb\"]\n",
    "\n",
    "# List of test audio files (paths from your dataset)\n",
    "test_audio_files = [\n",
    "    \"dataset_amazigh/wav/conv_wav/S1_conv_37.wav\",\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\"\n",
    "]\n",
    "\n",
    "# Function to resample audio to 16kHz if necessary\n",
    "def resample_audio(audio_path, target_sr=16000):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio)\n",
    "    return audio, target_sr\n",
    "\n",
    "# Translate and display the results\n",
    "for audio_file in test_audio_files:\n",
    "    print(f\"Audio file: {audio_file}\")\n",
    "    display(Audio(audio_file, rate=16000, autoplay=False, normalize=True))\n",
    "\n",
    "    for tgt_lang in tgt_langs:\n",
    "        # Resample audio if necessary\n",
    "        audio, sr = resample_audio(audio_file)\n",
    "        audio_file_resampled = audio_file.replace('.wav', '_16k.wav')\n",
    "        torchaudio.save(audio_file_resampled, audio, sr)\n",
    "\n",
    "        # Perform translation\n",
    "        text_output, _ = translator.predict(\n",
    "            input=audio_file_resampled,\n",
    "            task_str=\"s2tt\",\n",
    "            tgt_lang=tgt_lang,\n",
    "        )\n",
    "\n",
    "        print(f\"Translated text in {tgt_lang}: {text_output[0]}\")\n",
    "        print()\n",
    "\n",
    "        # Optionally save the translated audio if needed\n",
    "        # This part is a placeholder as it requires a TTS system to generate speech from text\n",
    "        # out_file = f\"/path/to/save/translated_{tgt_lang}_{audio_file.split('/')[-1]}\"\n",
    "        # torchaudio.save(out_file, translated_speech, sample_rate=16000)\n",
    "\n",
    "        # Display the translated audio if available\n",
    "        # audio_play = Audio(out_file, rate=16000, autoplay=False, normalize=True)\n",
    "        # display(audio_play)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02c94c37-76f4-4045-aabf-3e5fa1e8edb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: absl-py in /home/user/miniconda/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/user/miniconda/lib/python3.9/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/user/miniconda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/user/miniconda/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/user/miniconda/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/user/miniconda/lib/python3.9/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/user/miniconda/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24956 sha256=d19101657eee636e02a0ddb299a861f05bdbc4d492e962ada156c25dace89390\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: nltk, rouge_score\n",
      "Successfully installed nltk-3.8.1 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f5dd7e7c-e9d8-4d55-a935-b7a1b1254d65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_36langs. Set `force` to `True` to download again.\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "2024-05-19 06:54:28,117 INFO -- absl: Using default tokenizer.\n",
      "2024-05-19 06:54:28,431 INFO -- absl: Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: dataset_amazigh/wav/conv_wav/S3_conv_1.wav\n",
      "Translated text in arb: السلام عليكم من كان تيجيت؟\n",
      "WER: 1.2500\n",
      "CER: 1.0000\n",
      "BLEU: 0.0000\n",
      "\n",
      "Audio file: dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n",
      "Translated text in arb: لست بحاجة لتعاونك.\n",
      "WER: 1.0000\n",
      "CER: 0.8750\n",
      "BLEU: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from seamless_communication.inference import Translator\n",
    "from seamless_communication.streaming.dataloaders.s2tt import SileroVADSilenceRemover\n",
    "import evaluate\n",
    "\n",
    "# Initialize the Translator object with the correct parameters\n",
    "translator = Translator(\n",
    "    model_name_or_card=\"seamlessM4T_v2_large\",  # Use an appropriate model name if needed\n",
    "    vocoder_name_or_card=\"vocoder_36langs\",  # Assuming this is the correct vocoder for the model\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Define target languages\n",
    "tgt_langs = [\"arb\"]\n",
    "\n",
    "# Load evaluation metrics using the `evaluate` library\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "# List of test audio files \n",
    "test_audio_files = [\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\",\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\"\n",
    "]\n",
    "\n",
    "# Ground truth translations ( actual translations from CSV)\n",
    "ground_truth_translations = {\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\": \"هل تحتاج إلى مساعدة؟\",  \n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\": \"نحن هنا للمساعدة\"  \n",
    "}\n",
    "\n",
    "# Function to resample audio to 16kHz if necessary\n",
    "def resample_audio(audio_path, target_sr=16000):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio)\n",
    "    return audio, target_sr\n",
    "\n",
    "# Function to perform direct Amazigh-to-Arabic translation using the Translator\n",
    "def translate_audio(audio_path, tgt_lang):\n",
    "    # Resample audio if necessary\n",
    "    audio, sr = resample_audio(audio_path)\n",
    "    audio_file_resampled = audio_path.replace('.wav', '_16k.wav')\n",
    "    torchaudio.save(audio_file_resampled, audio, sr)\n",
    "    \n",
    "    # Perform translation\n",
    "    text_output, _ = translator.predict(\n",
    "        input=audio_file_resampled,\n",
    "        task_str=\"s2tt\",  # speech-to-text-to-text translation\n",
    "        tgt_lang=tgt_lang\n",
    "    )\n",
    "    return text_output[0]\n",
    "\n",
    "# Translate and display the results\n",
    "for audio_file in test_audio_files:\n",
    "    for tgt_lang in tgt_langs:\n",
    "        # Perform translation\n",
    "        translation = translate_audio(audio_file, tgt_lang)\n",
    "        \n",
    "        # Ensure translation is a string and not CString\n",
    "        translation = str(translation)\n",
    "\n",
    "        # Compute and print evaluation metrics using ground truth translations\n",
    "        label_str = ground_truth_translations[audio_file]  # Retrieve the actual reference text\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        wer = wer_metric.compute(predictions=[translation], references=[label_str])\n",
    "        cer = cer_metric.compute(predictions=[translation], references=[label_str])\n",
    "        bleu = bleu_metric.compute(predictions=[translation], references=[[label_str]])\n",
    "        rouge = rouge_metric.compute(predictions=[translation], references=[label_str])\n",
    "        meteor = meteor_metric.compute(predictions=[translation], references=[label_str])\n",
    "        ter = ter_metric.compute(predictions=[translation], references=[label_str])\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Audio file: {audio_file}\")\n",
    "        print(f\"Translated text in {tgt_lang}: {translation}\")\n",
    "        print(f\"WER: {wer:.4f}\")\n",
    "        print(f\"CER: {cer:.4f}\")\n",
    "        print(f\"BLEU: {bleu['bleu']:.4f}\")\n",
    "       \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "792e1fd4-d1ab-4dc5-abf1-d6573029dea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_36langs. Set `force` to `True` to download again.\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: dataset_amazigh/wav/conv_wav/S3_conv_1.wav\n",
      "Translated text in arb: السلام عليكم من كان تيجيت؟\n",
      "WER: 1.2500\n",
      "\n",
      "Audio file: dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\n",
      "Translated text in arb: لست بحاجة لتعاونك.\n",
      "WER: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "from seamless_communication.inference import Translator\n",
    "import evaluate\n",
    "\n",
    "# Initialize the Translator object with the correct parameters\n",
    "translator = Translator(\n",
    "    model_name_or_card=\"seamlessM4T_v2_large\",  # Use an appropriate model name if needed\n",
    "    vocoder_name_or_card=\"vocoder_36langs\",  # Assuming this is the correct vocoder for the model\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Define target language\n",
    "tgt_lang = \"arb\"\n",
    "\n",
    "# Load WER evaluation metric using the `evaluate` library\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# List of test audio files (paths from your dataset)\n",
    "test_audio_files = [\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\",\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\"\n",
    "]\n",
    "\n",
    "# Ground truth translations (replace these with actual translations from your CSV)\n",
    "ground_truth_translations = {\n",
    "    \"dataset_amazigh/wav/conv_wav/S3_conv_1.wav\": \"هل تحتاج إلى مساعدة؟\",  # Replace with actual Arabic translation\n",
    "    \"dataset_amazigh/wav/rescue_wav/S1_resc_2.wav\": \"نحن هنا للمساعدة\"  # Replace with actual Arabic translation\n",
    "}\n",
    "\n",
    "# Function to resample audio to 16kHz if necessary\n",
    "def resample_audio(audio_path, target_sr=16000):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio)\n",
    "    return audio, target_sr\n",
    "\n",
    "# Function to perform direct Amazigh-to-Arabic translation using the Translator\n",
    "def translate_audio(audio_path, tgt_lang):\n",
    "    # Resample audio if necessary\n",
    "    audio, sr = resample_audio(audio_path)\n",
    "    audio_file_resampled = audio_path.replace('.wav', '_16k.wav')\n",
    "    torchaudio.save(audio_file_resampled, audio, sr)\n",
    "    \n",
    "    # Perform translation\n",
    "    text_output, _ = translator.predict(\n",
    "        input=audio_file_resampled,\n",
    "        task_str=\"s2tt\",  # speech-to-text-to-text translation\n",
    "        tgt_lang=tgt_lang\n",
    "    )\n",
    "    return str(text_output[0])  # Ensure translation is a string\n",
    "\n",
    "# Translate and display the results\n",
    "for audio_file in test_audio_files:\n",
    "    # Perform translation\n",
    "    translation = translate_audio(audio_file, tgt_lang)\n",
    "    \n",
    "    # Compute and print WER using ground truth translations\n",
    "    label_str = ground_truth_translations[audio_file]  # Retrieve the actual reference text\n",
    "    wer = wer_metric.compute(predictions=[translation], references=[label_str])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Audio file: {audio_file}\")\n",
    "    print(f\"Translated text in {tgt_lang}: {translation}\")\n",
    "    print(f\"WER: {wer:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd9d9f-f175-4404-91a0-63399c66b780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
